{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preprocessor_config.json: 100%|██████████| 290/290 [00:00<00:00, 96.7kB/s]\n",
      "`huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\heeji\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n"
     ]
    }
   ],
   "source": [
    "from detr_model import DETR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. 셀의 코드를 검토하여 오류의 가능한 원인을 식별하세요. 자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'> 여기 </a> 를 클릭하세요. 자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DETR 모델 훈련, epoch=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DetrForObjectDetection were not initialized from the model checkpoint at facebook/detr-resnet-50 and are newly initialized because the shapes did not match:\n",
      "- class_labels_classifier.weight: found shape torch.Size([92, 256]) in the checkpoint and torch.Size([5, 256]) in the model instantiated\n",
      "- class_labels_classifier.bias: found shape torch.Size([92]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "\n",
      "  | Name  | Type                   | Params\n",
      "-------------------------------------------------\n",
      "0 | model | DetrForObjectDetection | 41.5 M\n",
      "-------------------------------------------------\n",
      "41.3 M    Trainable params\n",
      "222 K     Non-trainable params\n",
      "41.5 M    Total params\n",
      "166.040   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\heeji\\anaconda3\\envs\\yolo8\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:01<00:01,  1.22s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\heeji\\anaconda3\\envs\\yolo8\\lib\\site-packages\\pytorch_lightning\\utilities\\data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\heeji\\anaconda3\\envs\\yolo8\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199: 100%|██████████| 90/90 [00:25<00:00,  3.49it/s, v_num=3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199: 100%|██████████| 90/90 [00:30<00:00,  2.99it/s, v_num=3]\n"
     ]
    }
   ],
   "source": [
    "model = DETR()\n",
    "train_data=\"./COCO_format/\"\n",
    "model.train(train_data,epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련한 모델 load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DetrForObjectDetection\n",
    "trained=DetrForObjectDetection.from_pretrained(\"detr_model\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Detections(xyxy=array([[     296.57,      174.35,       340.4,      358.55],\n",
       "       [     295.84,      169.61,      340.96,      349.03],\n",
       "       [     272.38,      108.55,      302.61,      188.26],\n",
       "       [      292.3,      194.75,      327.11,      362.72],\n",
       "       [     321.13,      237.58,      365.38,      415.84]], dtype=float32), mask=None, confidence=array([     0.9314,     0.72505,     0.99667,     0.99844,     0.99829], dtype=float32), class_id=array([3, 3, 1, 3, 3]), tracker_id=None)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\"./COCO_format/test/left1009_jpg.rf.6d1f7225244659b4968d37642eef15a0.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bounding box 표시하고 별도로 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_save(img):\n",
    "    import supervision as sv\n",
    "    import cv2\n",
    "    \n",
    "    image = cv2.imread(img)\n",
    "    print(img)\n",
    "    classes = [\"WaterTank\",\"Water Tank\",\"lift\", \"worker\"]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        detections = model.predict(img)\n",
    "        labels = [f\"{classes[class_id]} {confidence:0.2f}\" for _, _, confidence, class_id, _ in detections]\n",
    "        \n",
    "    box_annotator = sv.BoxAnnotator()\n",
    "    annotated_frame = box_annotator.annotate(scene=image.copy(), detections=detections, labels=labels)\n",
    "    sv.plot_image(annotated_frame)\n",
    "    \n",
    "    parts = img.split(\"_jpg.rf.\")\n",
    "    parts = parts[0].split(\"/\")[-1]\n",
    "    output_path=\"./DETR_result/\"+parts+\".png\"\n",
    "    print(output_path)\n",
    "    cv2.imwrite(output_path, annotated_frame)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "test_list=os.listdir('./COCO_format/test1')\n",
    "    \n",
    "for i in range(len(test_list)):\n",
    "    predict_and_save('./test/images/'+test_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## map 구하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련한 모델을 test 데이터에 적용하여 딕셔너리 형태로 supervision detection들을 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list=os.listdir('./COCO_format/test1')\n",
    "pred={}\n",
    "\n",
    "for i in range(len(test_list)):\n",
    "    \n",
    "    parts = test_list[i].split(\"_jpg.rf.\")\n",
    "    parts = parts[0].split(\"/\")[-1]\n",
    "    img = './COCO_format/test1/'+test_list[i]\n",
    "    pred[test_list[i]]=((model.predict(img)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 아래의 코드를 실행하여 ground_truth를 딕셔너리 형태로(dataset.annotations) 얻을 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import supervision as sv\n",
    "import numpy as np\n",
    "from core_dataset import DetectionDataset\n",
    "\n",
    "dataset = DetectionDataset.from_coco(\"./COCO_format/test1\",\"./COCO_format/test/_annotations.coco.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pred와 dataset.annotations의 정렬이 달라서 키 값을 기준으로 재정렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['left1009_jpg.rf.6d1f7225244659b4968d37642eef15a0.jpg', 'left1011_jpg.rf.80329927f937d116686bc3a6f913274a.jpg', 'left1014_jpg.rf.df67534395da28c5ebd381b83131efdd.jpg', 'left1038_jpg.rf.c92010b21ee8803474021aea5c6607b7.jpg', 'left1040_jpg.rf.63fc982a793fb0695a333bed41272fd2.jpg', 'left1047_jpg.rf.08068fca0ef4edf6c17ec9f14803c585.jpg', 'left1060_jpg.rf.19d990c1f9e55c5b644c702c6d6ffc82.jpg', 'left1077_jpg.rf.448534dd2757b590bf8c97023c04c48d.jpg', 'left1107_jpg.rf.34b2a89f4e1d3ea508927bbc5f126088.jpg', 'left1123_jpg.rf.e4144494a49e5a06a409ba150f475073.jpg', 'left1124_jpg.rf.d67db28622ad41ed2aa2dcff355806c6.jpg', 'left1136_jpg.rf.cb64a681b0e70e5a76e58c357b6573e0.jpg', 'left1138_jpg.rf.3afa657e912e7024e38d67ad2ff3332b.jpg', 'left1151_jpg.rf.7674c5e1bd7b8adcc196ef888742a903.jpg', 'left1152_jpg.rf.ca2b54f9023fa0025e8766a9026c96e5.jpg', 'left1274_jpg.rf.93315ad433438e1b9639677bd31666b5.jpg', 'left1302_jpg.rf.2b63af6730db97be9dee80e4cd879d86.jpg', 'left188_jpg.rf.1492c34da3cb9cb5b3c290f2755b0057.jpg', 'left235_jpg.rf.268e41f58e92c1eebb3a4da8ddaedcf7.jpg', 'left265_jpg.rf.b8f9ffe3d52ab44b66735bfb913b7f14.jpg', 'left268_jpg.rf.f938a95f3ec2a6bd011e38c31c58de73.jpg', 'left451_jpg.rf.d8eb1587a3fb591f0b094f099bfb6286.jpg', 'left453_jpg.rf.f20cf8efa15378d89719fc14834c9d4f.jpg', 'left476_jpg.rf.e5ac45679552d5fabd623036fd1efe4f.jpg', 'left482_jpg.rf.e7c0a813b806309f7660d39889fa6611.jpg']\n"
     ]
    }
   ],
   "source": [
    "sorted_keys1 = sorted(pred.keys())\n",
    "print(sorted_keys1)\n",
    "\n",
    "sorted_keys2 = sorted(dataset.annotations.keys())\n",
    "print(sorted_keys2) \n",
    "\n",
    "pred = {key: pred[key] for key in sorted_keys1}\n",
    "anno = {key: dataset.annotations[key] for key in sorted_keys2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['left1009_jpg.rf.6d1f7225244659b4968d37642eef15a0.jpg', 'left1011_jpg.rf.80329927f937d116686bc3a6f913274a.jpg', 'left1014_jpg.rf.df67534395da28c5ebd381b83131efdd.jpg', 'left1038_jpg.rf.c92010b21ee8803474021aea5c6607b7.jpg', 'left1040_jpg.rf.63fc982a793fb0695a333bed41272fd2.jpg', 'left1047_jpg.rf.08068fca0ef4edf6c17ec9f14803c585.jpg', 'left1060_jpg.rf.19d990c1f9e55c5b644c702c6d6ffc82.jpg', 'left1077_jpg.rf.448534dd2757b590bf8c97023c04c48d.jpg', 'left1107_jpg.rf.34b2a89f4e1d3ea508927bbc5f126088.jpg', 'left1123_jpg.rf.e4144494a49e5a06a409ba150f475073.jpg', 'left1124_jpg.rf.d67db28622ad41ed2aa2dcff355806c6.jpg', 'left1136_jpg.rf.cb64a681b0e70e5a76e58c357b6573e0.jpg', 'left1138_jpg.rf.3afa657e912e7024e38d67ad2ff3332b.jpg', 'left1151_jpg.rf.7674c5e1bd7b8adcc196ef888742a903.jpg', 'left1152_jpg.rf.ca2b54f9023fa0025e8766a9026c96e5.jpg', 'left1274_jpg.rf.93315ad433438e1b9639677bd31666b5.jpg', 'left1302_jpg.rf.2b63af6730db97be9dee80e4cd879d86.jpg', 'left188_jpg.rf.1492c34da3cb9cb5b3c290f2755b0057.jpg', 'left235_jpg.rf.268e41f58e92c1eebb3a4da8ddaedcf7.jpg', 'left265_jpg.rf.b8f9ffe3d52ab44b66735bfb913b7f14.jpg', 'left268_jpg.rf.f938a95f3ec2a6bd011e38c31c58de73.jpg', 'left451_jpg.rf.d8eb1587a3fb591f0b094f099bfb6286.jpg', 'left453_jpg.rf.f20cf8efa15378d89719fc14834c9d4f.jpg', 'left476_jpg.rf.e5ac45679552d5fabd623036fd1efe4f.jpg', 'left482_jpg.rf.e7c0a813b806309f7660d39889fa6611.jpg'])\n",
      "dict_keys(['left1009_jpg.rf.6d1f7225244659b4968d37642eef15a0.jpg', 'left1011_jpg.rf.80329927f937d116686bc3a6f913274a.jpg', 'left1014_jpg.rf.df67534395da28c5ebd381b83131efdd.jpg', 'left1038_jpg.rf.c92010b21ee8803474021aea5c6607b7.jpg', 'left1040_jpg.rf.63fc982a793fb0695a333bed41272fd2.jpg', 'left1047_jpg.rf.08068fca0ef4edf6c17ec9f14803c585.jpg', 'left1060_jpg.rf.19d990c1f9e55c5b644c702c6d6ffc82.jpg', 'left1077_jpg.rf.448534dd2757b590bf8c97023c04c48d.jpg', 'left1107_jpg.rf.34b2a89f4e1d3ea508927bbc5f126088.jpg', 'left1123_jpg.rf.e4144494a49e5a06a409ba150f475073.jpg', 'left1124_jpg.rf.d67db28622ad41ed2aa2dcff355806c6.jpg', 'left1136_jpg.rf.cb64a681b0e70e5a76e58c357b6573e0.jpg', 'left1138_jpg.rf.3afa657e912e7024e38d67ad2ff3332b.jpg', 'left1151_jpg.rf.7674c5e1bd7b8adcc196ef888742a903.jpg', 'left1152_jpg.rf.ca2b54f9023fa0025e8766a9026c96e5.jpg', 'left1274_jpg.rf.93315ad433438e1b9639677bd31666b5.jpg', 'left1302_jpg.rf.2b63af6730db97be9dee80e4cd879d86.jpg', 'left188_jpg.rf.1492c34da3cb9cb5b3c290f2755b0057.jpg', 'left235_jpg.rf.268e41f58e92c1eebb3a4da8ddaedcf7.jpg', 'left265_jpg.rf.b8f9ffe3d52ab44b66735bfb913b7f14.jpg', 'left268_jpg.rf.f938a95f3ec2a6bd011e38c31c58de73.jpg', 'left451_jpg.rf.d8eb1587a3fb591f0b094f099bfb6286.jpg', 'left453_jpg.rf.f20cf8efa15378d89719fc14834c9d4f.jpg', 'left476_jpg.rf.e5ac45679552d5fabd623036fd1efe4f.jpg', 'left482_jpg.rf.e7c0a813b806309f7660d39889fa6611.jpg'])\n"
     ]
    }
   ],
   "source": [
    "print(pred.keys())\n",
    "print(anno.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 재정렬된 딕셔너리들의 value값들을 MeanAveragePrecision 모듈에 입력하여 map 도출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map(50_95): 0.6242863729761101\n",
      "map(50): 0.9273284820217941\n",
      "map(75): 0.6827641881872483\n",
      "map(per_class): [[      0.978       0.978       0.978       0.978       0.978     0.92652     0.85585     0.71953     0.35999           0]\n",
      " [    0.87666     0.86565     0.81233     0.69797     0.64172     0.43901     0.23858     0.11132    0.044024   0.0065656]]\n"
     ]
    }
   ],
   "source": [
    "def callback(image) -> sv.Detections:\n",
    "    result = model.predict(image)\n",
    "    return (result)\n",
    "\n",
    "from detection import MeanAveragePrecision\n",
    "mean_average_precision = MeanAveragePrecision.from_detections(pred.values(),anno.values())\n",
    "\n",
    "print(\"map(50_95):\", mean_average_precision.map50_95)\n",
    "print(\"map(50):\",mean_average_precision.map50)\n",
    "print(\"map(75):\",mean_average_precision.map75)\n",
    "print(\"map(per_class):\",mean_average_precision.per_class_ap50_95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/roboflow/supervision"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
